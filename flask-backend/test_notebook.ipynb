{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database...\n",
      "List all collections:\n",
      " ['test']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import pandas as pd\n",
    "from milvus import create_collection\n",
    "from pymilvus import connections, Collection, MilvusException, utility, CollectionSchema, FieldSchema, DataType\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from itertools import chain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_docs):\n",
    "    text_all = []\n",
    "    for pdf in pdf_docs:\n",
    "        text = \"\"\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        text_all.append(text)\n",
    "    return text_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embbedings(text, **kwargs):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = ['M:\\AI\\hackhaton-project\\kodeks_karny.pdf', 'M:\\AI\\hackhaton-project\\konstytucja.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleFiles(files):\n",
    "    texts = get_pdf_text(files)\n",
    "    chunks = processPdfs(texts)\n",
    "    return chunks\n",
    "    #get_vectore_store(chunks)\n",
    "\n",
    "def get_pdf_text(pdf_docs):\n",
    "    text_all = []\n",
    "    for pdf in pdf_docs:\n",
    "        text = \"\"\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        text_all.append(text)\n",
    "    return text_all\n",
    "\n",
    "def processPdfs(raw_texts):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks_array = []\n",
    "    for raw_text in raw_texts:\n",
    "        chunks = text_splitter.split_text(raw_text)\n",
    "        print(len(chunks))\n",
    "        chunks_array.append(chunks)\n",
    "    return chunks_array\n",
    "\n",
    "def get_embbedings(text, **kwargs):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "    \n",
    "\n",
    "def get_vectore_store(chunks_array):\n",
    "    collection = create_collection('test')\n",
    "    inserted_rows = 0\n",
    "    for chunks in chunks_array:\n",
    "        data = [{'text': chunk, 'vector': get_embbedings(chunk)} for chunk in chunks]\n",
    "        df_efficient = pd.DataFrame(data)\n",
    "        try:\n",
    "            mr = collection.insert(df_efficient)\n",
    "            inserted_rows += mr.insert_count\n",
    "            print(mr)\n",
    "        except MilvusException as e:\n",
    "            print(e)\n",
    "    print(inserted_rows)        \n",
    "    nlist = 4 * int(np.round(np.sqrt(inserted_rows)))\n",
    "    print(nlist)\n",
    "    \n",
    "    index_params = {\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\n",
    "            \"nlist\": nlist\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        collection.create_index(\n",
    "            field_name=\"vector\",\n",
    "            index_params=index_params,\n",
    "            index_name=\"SimpleIndex\"\n",
    "        )\n",
    "        #collection.load(replica_number=1)\n",
    "    except MilvusException as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "chunks = handleFiles(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_chunks_array(chunks_array):\n",
    "    return list(chain.from_iterable(chunks_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_chunks = flatten_chunks_array(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(name: str):\n",
    "    id = FieldSchema(\n",
    "        name=\"id\",\n",
    "        dtype=DataType.INT64,\n",
    "        is_primary=True,\n",
    "        auto_id=True\n",
    "    )\n",
    "    text = FieldSchema(\n",
    "        name=\"text\",\n",
    "        dtype=DataType.VARCHAR,\n",
    "        max_length=1300\n",
    "    )\n",
    "    vector = FieldSchema(\n",
    "        name=\"vector\",\n",
    "        dtype=DataType.FLOAT_VECTOR,\n",
    "        dim=1536\n",
    "    )\n",
    "    schema = CollectionSchema(\n",
    "        fields=[id, text, vector],\n",
    "        description=\"Test collection\",\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    new_collection = Collection(\n",
    "        name=name,\n",
    "        schema=schema,\n",
    "        using='default',\n",
    "        shards_num=4\n",
    "    )\n",
    "    return new_collection\n",
    "\n",
    "collection = create_collection('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = create_collection('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    try:\n",
    "        data = {'text': chunk, 'vector': get_embbedings(chunk)}\n",
    "        df_efficient = pd.DataFrame([data])\n",
    "        mr = collection.insert(df_efficient)\n",
    "        return mr.insert_count\n",
    "    except MilvusException as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = create_collection('test')\n",
    "inserted_rows_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertion_lock = Lock()\n",
    "\n",
    "def execute_concurently(flatten_chunks, num_workers):\n",
    "    global inserted_rows_count\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_chunk = {executor.submit(process_chunk, chunk): chunk for chunk in flatten_chunks}\n",
    "    \n",
    "        for future in as_completed(future_to_chunk):\n",
    "            chunk = future_to_chunk[future]\n",
    "            try:\n",
    "                inserted_count = future.result()\n",
    "                with insertion_lock:\n",
    "                    inserted_rows_count += inserted_count\n",
    "            except Exception as exc:\n",
    "                print(f'Chunk {chunk} generated an exception: {exc}')\n",
    "    print(f\"Total inserted rows: {inserted_rows_count}\")\n",
    "    return inserted_rows_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inserted rows: 461\n"
     ]
    }
   ],
   "source": [
    "execute_concurently(flatten_chunks, num_workers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SimpleIndex']\n",
      "jest index\n"
     ]
    }
   ],
   "source": [
    "indexes = utility.list_indexes(collection_name=\"test\")\n",
    "print(indexes)\n",
    "if(len(indexes) > 0):\n",
    "    print(\"jest index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cześć Mateusz! Miło mi Cię poznać. Jak się masz?\n",
      "Masz na imię Mateusz. Czy masz jakieś inne pytania?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    #streaming=True,\n",
    "    #callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "template = \"\"\"Talk like you are pirat.\n",
    "Current conversation:\n",
    "{history}\n",
    "Me:{input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"history\", \"input\"]\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    "    prompt=PROMPT\n",
    ")\n",
    "\n",
    "ai_response = conversation.predict(\n",
    "    input=\"Mam na imie mateusz\",\n",
    ")\n",
    "\n",
    "ai_response2 = conversation.predict(\n",
    "    input=\"Jak mam na imie?\"\n",
    ")\n",
    "\n",
    "# conversation.predict(\n",
    "    \n",
    "# )\n",
    "\n",
    "# print(conversation.memory)\n",
    "# print(\"\\n\")\n",
    "# #conversation.memory.clear()\n",
    "# print(conversation.memory)\n",
    "\n",
    "print(ai_response)\n",
    "print(ai_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Mam na imie mateusz'), AIMessage(content='Witam w czym moge pomoc'), HumanMessage(content='Jak mam na imie'), AIMessage(content='Mateusz')]\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "history.append(HumanMessage(content=\"Mam na imie mateusz\"))\n",
    "history.append(AIMessage(\"Witam w czym moge pomoc\"))\n",
    "history.append(HumanMessage(content=\"Jak mam na imie\"))\n",
    "history.append(AIMessage(\"Mateusz\"))\n",
    "# history_string = \"\\n\".join(history)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masz na imię Mateusz. Czy mogę Ci pomóc w czymś jeszcze?\n",
      "['Human: Mam na imie Mateusz', 'AI: Miło mi Cię poznać, Mateuszu! W czym mogę Ci dzisiaj pomóc?', 'Human: Jak mam na imie', 'AI: Masz na imię Mateusz. Czy mogę Ci pomóc w czymś jeszcze?']\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "history = []\n",
    "history_string = \"\\n\".join(history)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Jestes pomocnym asystentem\"},\n",
    "        {\"role\": \"system\", \"content\": history_string},\n",
    "        #{\"role\": \"assistant\", \"content\": \"Jacek Soplica dostał od Stolnika czarna polewke i odrzucil zareczyny dla swojej córki\"},\n",
    "        {\"role\": \"user\", \"content\": \"Mam na imie Mateusz\"}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "history.append(\"Human: Mam na imie Mateusz\")\n",
    "history.append(\"AI: \" + answer)\n",
    "history_string = \"\\n\".join(history)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Jestes pomocnym asystentem\"},\n",
    "        {\"role\": \"system\", \"content\": history_string},\n",
    "        #{\"role\": \"assistant\", \"content\": \"Jacek Soplica dostał od Stolnika czarna polewke i odrzucil zareczyny dla swojej córki\"},\n",
    "        {\"role\": \"user\", \"content\": \"jak mam na imie?\"}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "history.append(\"Human: Jak mam na imie\")\n",
    "history.append(\"AI: \" + answer)\n",
    "history_string = \"\\n\".join(history)\n",
    "\n",
    "print(answer)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusException, utility, Collection\n",
    "\n",
    "def get_similarieties(text: str, collection: str, returned_chunks: int):\n",
    "    output_chunks = []\n",
    "    collection = Collection(name=collection)\n",
    "    \n",
    "    vector_prompt = get_embbedings(text)\n",
    "    \n",
    "    search_params = {\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"offset\": 0,\n",
    "            \"ignore_growing\": False,\n",
    "            \"params\": {\"nprobe\": 80} #pwinno byc mniejsze niż nlist z indexu kolekcji\n",
    "        }\n",
    "    \n",
    "    result = collection.search(\n",
    "            data=[vector_prompt], #zmienna do ktorej bedzie przeszukanie\n",
    "            anns_field=\"vector\", #nazwa kolumny w kolekcji przechowujaca vektory\n",
    "            param=search_params, #parametry wyszukiwania deklarowane wczesniej\n",
    "            limit= returned_chunks, #ile ma zwrocic najbardziej podobnych\n",
    "            output_fields=['text'] #kolumna ktora bedzie zwracana\n",
    "        )\n",
    "    \n",
    "    for ids, hit in enumerate(result[0]):\n",
    "        output_chunks.append(hit.entity.text)\n",
    "        print(hit.entity.text)\n",
    "        \n",
    "    return output_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dopuszcza się gwałtownego zamachu na jednostkę Sił Zbrojnych Rzeczypospolitej \n",
      "Polskiej, niszczy lub uszkadza obiekt albo urządzenie o  znaczeniu obronnym,  \n",
      "podlega karze pozbawienia wolności od roku do lat 10.  \n",
      "§ 2. Jeżeli następst wem czynu jest śmierć człowieka lub ciężki uszczerbek na \n",
      "zdrowiu wielu osób, sprawca  \n",
      "podlega karze pozbawienia wolności  od lat 2 do 15.  \n",
      "§ 3. Kto czyni przygotowania do przestępstwa określonego w  § 1, \n",
      "podlega karze pozbawienia wolności do lat 3.  \n",
      "§ 4. W sprawie o  przestępstwo określone w  § 1–3 sąd może orzec przepadek \n",
      "przedmiotów również wtedy, gdy przedmioty nie stanowią własności sprawcy.  \n",
      "Art. 141. § 1. Kto, będąc obywatelem polskim, przyjmuje bez zgody właściwego \n",
      "organu obowiązki wojskowe w  obcym wojsku lu b w obcej organizacji wojskowej,  \n",
      "podlega karze pozbawienia wolności od 3  miesięcy do lat 5.  ©Kancelaria Sejmu     s. 67/146 \n",
      "   \n",
      "2024 -01-17 \n",
      " § 2. Kto przyjmuje obowiązki w  zakazanej przez prawo międzynarodowe\n",
      "['dopuszcza się gwałtownego zamachu na jednostkę Sił Zbrojnych Rzeczypospolitej \\nPolskiej, niszczy lub uszkadza obiekt albo urządzenie o  znaczeniu obronnym,  \\npodlega karze pozbawienia wolności od roku do lat 10.  \\n§ 2. Jeżeli następst wem czynu jest śmierć człowieka lub ciężki uszczerbek na \\nzdrowiu wielu osób, sprawca  \\npodlega karze pozbawienia wolności  od lat 2 do 15.  \\n§ 3. Kto czyni przygotowania do przestępstwa określonego w  § 1, \\npodlega karze pozbawienia wolności do lat 3.  \\n§ 4. W sprawie o  przestępstwo określone w  § 1–3 sąd może orzec przepadek \\nprzedmiotów również wtedy, gdy przedmioty nie stanowią własności sprawcy.  \\nArt. 141. § 1. Kto, będąc obywatelem polskim, przyjmuje bez zgody właściwego \\norganu obowiązki wojskowe w  obcym wojsku lu b w obcej organizacji wojskowej,  \\npodlega karze pozbawienia wolności od 3  miesięcy do lat 5.  ©Kancelaria Sejmu     s. 67/146 \\n   \\n2024 -01-17 \\n § 2. Kto przyjmuje obowiązki w  zakazanej przez prawo międzynarodowe']\n"
     ]
    }
   ],
   "source": [
    "print(get_similarieties(\n",
    "    text=\"Artukuł o napadzie z bronią\",\n",
    "    collection=\"test\",\n",
    "    returned_chunks=1\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
